{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b80f397",
   "metadata": {},
   "source": [
    "### Read CSV with top 100 songs bilboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bilboard_df1 = pd.read_csv(\"../datas/top100.csv\", low_memory=False)\n",
    "bilboard_df2 = pd.read_csv(\"../datas/top100oct.csv\", low_memory=False)\n",
    "bilboard_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b82694",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c023db",
   "metadata": {},
   "source": [
    "##### Compare 2 DF and print the duplicate items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515640ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_bilboards():\n",
    "    for song, songs in enumerate(bilboard_df1['title']):\n",
    "        for sound, sounds in enumerate(bilboard_df2['title']):\n",
    "            if songs == sounds:\n",
    "                playlist = {\n",
    "                    \"Musica\": bilboard_df1['title'][sound],\n",
    "                    \"Artista\": bilboard_df2['artist'][sound]\n",
    "                }\n",
    "                print(playlist)\n",
    "duplicate_bilboards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5034bd6",
   "metadata": {},
   "source": [
    "##### SparkSession create and Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "path = \"../datas/top100oct.csv\"\n",
    "path2 = \"../datas/top100.csv\"\n",
    "bilboard_df1_spark = spark.read.csv(path)\n",
    "bilboard_df1_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ecb0a",
   "metadata": {},
   "source": [
    "##### Get all data with Schema context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed88c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark = spark.read.format(\"csv\").option(\"inferSchema\", \"True\").option(\"header\", \"True\").csv(path)\n",
    "bilboard_df1_spark.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df2_spark = spark.read.format(\"csv\").option(\"inferSchema\", \"True\").option(\"header\", \"True\").csv(path2)\n",
    "bilboard_df2_spark.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81520ea",
   "metadata": {},
   "source": [
    "##### Show schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1755cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b2c51fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- featured_artist: string (nullable = true)\n",
      " |-- this_week_chart: integer (nullable = true)\n",
      " |-- last_week_chart: string (nullable = true)\n",
      " |-- peak: integer (nullable = true)\n",
      " |-- weeks_on_chart: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bilboard_df2_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bilboard_df1_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bilboard_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e4f85",
   "metadata": {},
   "source": [
    "##### Get first 5 lines with array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec859b6",
   "metadata": {},
   "source": [
    "##### Format to show better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67926f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bilboard_df1_spark.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e1a04d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilboard_df2_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed94556",
   "metadata": {},
   "source": [
    "##### Turn False inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark = spark.read.format(\"csv\").option(\"inferSchema\", \"False\").option(\"header\", \"True\").csv(path)\n",
    "bilboard_df1_spark.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bc329",
   "metadata": {},
   "source": [
    "##### Pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "bilboard_df1_spark.orderBy(expr(\"peak desc\")).show(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ee542",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark.orderBy(expr(\"artist asc\")).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3719203",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bilboard_df1_spark.select(\"artist\").show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7fff3",
   "metadata": {},
   "source": [
    "#### Using filter + order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad43bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bilboard_df1_spark.filter(\"this_week_chart > 4\").orderBy(expr(\"title asc\")).show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497416ef",
   "metadata": {},
   "source": [
    "##### Rename column in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb308f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilboard_df1_spark = bilboard_df1_spark.withColumnRenamed(\"artist\", \"Artista\").withColumnRenamed(\"title\", \"Musica\").withColumnRenamed(\n",
    "    \"this_week_chart\", \"TocadasNaSemana\").withColumnRenamed(\"last_week_chart\", \"TocadasSemanaPassada\").withColumnRenamed(\"peak\", \"Pico\").withColumnRenamed(\"weeks_on_chart\", \"GraficoSemanal\")\n",
    "bilboard_df1_spark.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f07f4",
   "metadata": {},
   "source": [
    "##### ADD Column in dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a5ffc1e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) peak#3378 missing from Musica#3401,Artista#3394,TocadasNaSemana#3408,TocadasSemanaPassada#3415,Pico#3422,GraficoSemanal#3429 in operator !Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, Pico#3422, GraficoSemanal#3429, (peak#3378 * 100) AS product#3481].;\n!Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, Pico#3422, GraficoSemanal#3429, (peak#3378 * 100) AS product#3481]\n+- Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, Pico#3422, weeks_on_chart#3344 AS GraficoSemanal#3429]\n   +- Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, peak#3343 AS Pico#3422, weeks_on_chart#3344]\n      +- Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, last_week_chart#3342 AS TocadasSemanaPassada#3415, peak#3343, weeks_on_chart#3344]\n         +- Project [Musica#3401, Artista#3394, this_week_chart#3341 AS TocadasNaSemana#3408, last_week_chart#3342, peak#3343, weeks_on_chart#3344]\n            +- Project [title#3339 AS Musica#3401, Artista#3394, this_week_chart#3341, last_week_chart#3342, peak#3343, weeks_on_chart#3344]\n               +- Project [title#3339, artist#3340 AS Artista#3394, this_week_chart#3341, last_week_chart#3342, peak#3343, weeks_on_chart#3344]\n                  +- Relation [title#3339,artist#3340,this_week_chart#3341,last_week_chart#3342,peak#3343,weeks_on_chart#3344] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ramon/github/Spark-Airflow/src_data_bilboards/bilboards.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramon/github/Spark-Airflow/src_data_bilboards/bilboards.ipynb#ch0000038?line=0'>1</a>\u001b[0m bilboard_df2_spark\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ramon/github/Spark-Airflow/src_data_bilboards/bilboards.ipynb#ch0000038?line=1'>2</a>\u001b[0m bilboard_df1_spark \u001b[39m=\u001b[39m bilboard_df1_spark\u001b[39m.\u001b[39;49mwithColumn(\u001b[39m\"\u001b[39;49m\u001b[39mproduct\u001b[39;49m\u001b[39m\"\u001b[39;49m, bilboard_df2_spark[\u001b[39m'\u001b[39;49m\u001b[39mpeak\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m*\u001b[39;49m\u001b[39m100\u001b[39;49m)\u001b[39m.\u001b[39mshow(\u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2478\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   2477\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcol should be Column\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) peak#3378 missing from Musica#3401,Artista#3394,TocadasNaSemana#3408,TocadasSemanaPassada#3415,Pico#3422,GraficoSemanal#3429 in operator !Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, Pico#3422, GraficoSemanal#3429, (peak#3378 * 100) AS product#3481].;\n!Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, Pico#3422, GraficoSemanal#3429, (peak#3378 * 100) AS product#3481]\n+- Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, Pico#3422, weeks_on_chart#3344 AS GraficoSemanal#3429]\n   +- Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, TocadasSemanaPassada#3415, peak#3343 AS Pico#3422, weeks_on_chart#3344]\n      +- Project [Musica#3401, Artista#3394, TocadasNaSemana#3408, last_week_chart#3342 AS TocadasSemanaPassada#3415, peak#3343, weeks_on_chart#3344]\n         +- Project [Musica#3401, Artista#3394, this_week_chart#3341 AS TocadasNaSemana#3408, last_week_chart#3342, peak#3343, weeks_on_chart#3344]\n            +- Project [title#3339 AS Musica#3401, Artista#3394, this_week_chart#3341, last_week_chart#3342, peak#3343, weeks_on_chart#3344]\n               +- Project [title#3339, artist#3340 AS Artista#3394, this_week_chart#3341, last_week_chart#3342, peak#3343, weeks_on_chart#3344]\n                  +- Relation [title#3339,artist#3340,this_week_chart#3341,last_week_chart#3342,peak#3343,weeks_on_chart#3344] csv\n"
     ]
    }
   ],
   "source": [
    "bilboard_df1_spark = bilboard_df1_spark.withColumn(\"product\", bilboard_df1_spark['peak']*100).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957e7bd",
   "metadata": {},
   "source": [
    "##### Drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0de71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilboard_df1_spark = bilboard_df1_spark.drop(\"Valor\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bilboard_df1_spark.show(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
